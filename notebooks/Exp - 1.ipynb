{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8247667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "a = 2\n",
    "b = 3\n",
    "\n",
    "print(a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb0b74bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !uv pip install langchain_community langchain langchain-cohere cohere tiktoken python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0132e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "cohere_api_key = os.environ[\"COHERE_API_KEY\"]\n",
    "huggingface_api_key = os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce8c5485",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'X'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohere_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c124bbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data ingestion \n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ea810f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader(\n",
    "    \"C:/Biprayan - 2/LLMops/project/data/demo.txt\",\n",
    "    autodetect_encoding= True\n",
    ")\n",
    "\n",
    "documents = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e227b53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Agentic Document Extraction: OCR to Understanding Content\\nAnirban Chakraborty\\nAnirban Chakraborty\\n4 min read\\nÂ·\\nAug 9, 2025\\n\\nPress enter or click to view image in full size\\nA diagram showing an early approach at ADE, which will likely evolve over time.\\n\\nThis article explores how traditional Optical Character Recognition (OCR) has evolved into advanced, generative AI-powered Agentic Document Extraction (ADE) systems â€” capable of genuinely understanding documents rather than simply reading text. Weâ€™ll discuss why OCR alone isnâ€™t sufficient anymore, how ADE addresses some of the shortcomings, and the real-world challenges teams face today.\\n\\nA Quick History of OCR & Document Parsing\\n\\nOCR has been the primary technique for extracting textual information from documents but had clear limitations:\\n\\n    Base OCR: Captures plain text but lacks context â€” it doesnâ€™t understand how information is related, nor its original layout.\\n    Form/Table Parsers: Extract structured data (key-value pairs, tables) but canâ€™t deeply validate information or understand overall page structure.\\n    Layout Parsers: Identify structures like headings and images but lack deeper reasoning capabilities (such as validating invoice amounts).\\n\\n    Despite significant improvements over the years, even modern parsers are fragile when document layouts change or unfamiliar types appear.\\n\\nWhy Base OCR Alone Isnâ€™t Enough\\n\\nBase OCR struggles with:\\n\\n    Complex layouts: Tables, handwritten notes, charts, or mixed formats often confuse template-driven OCR.\\n    Spatial awareness: OCR doesnâ€™t keep track of the original position of extracted data.\\n    Validation gaps: OCR canâ€™t automatically confirm totals or catch data errors without manual intervention / custom code.\\n    Manual review: Increasing complexity means higher costs and more human involvement, introducing a wider scope for errors (who can really concentrate the whole way through a 1000 page document?).\\n    Changing layouts: Fixed-template OCR systems fail or require updates whenever document designs change.\\n\\nPress enter or click to view image in full size\\nA diagram showing how traditional OCR systems typically work\\n\\nApplications that use OCR will normally build extensive amounts of custom code around the OCR to try and mitigate the limitations and protect from errors, however this leads to reduced agility and increased cost per document, and higher ongoing maintenance.\\n\\nTodayâ€™s applications need solutions capable of:\\n\\n    Combining text and visuals (e.g., barcodes, images, checkboxes).\\n    Maintaining spatial context â€” tracking exactly where data originates.\\n    Automatically checking data consistency, such as verifying totals or dates.\\n\\nIntroducing Agentic Document Extraction\\nGet Anirban Chakrabortyâ€™s stories in your inbox\\n\\nJoin Medium for free to get updates from this writer.\\n\\nADE surpasses OCR by intelligently understanding, validating, and structuring document information. ADE offers:\\n\\n    Visual Grounding: Every data point includes exact coordinates from its original document location. Example: When capturing a handwritten patient ID, ADE records precisely where it appeared on the page, making verification easy.\\n    Component Awareness: Automatically identifies and relates document elements (tables, images, form fields) without manual templates. Example: ADE recognizes separate sections on forms â€” such as client details, checklists, signatures â€” without predefined rules.\\n    Semantic Reasoning: Uses advanced language models to verify data accuracy, find anomalies, or fill missing fields intelligently. Example: ADE can detect mismatches, such as incomplete client IDs or incorrect policy numbers, and flag these automatically.\\n    Structured Outputs: Data is delivered in structured formats (JSON) aligned with predefined schemas, immediately usable for analytics, workflows and integrations.\\n\\nPress enter or click to view image in full size\\nA diagram showing an early approach at ADE, which will likely evolve over time.\\n\\nKey Challenges in Agentic Document Extraction\\n\\nImplementing ADE is powerful â€” but has some significant challenges to consider before embarking on a new solution in this field:\\n\\n    Complex DevOps: ADE involves multiple AI agents, making reproducibility and management difficult.\\n    Accuracy Drift: Model performance can degrade over time, requiring continuous monitoring.\\n    Lack of Domain-specific Data: Industries often struggle with insufficient labeled documents to train models effectively.\\n    Document Size Limits: Large documents exceed current AI model capabilities, complicating extraction.\\n\\nSome of these challenges are indeed cross-cutting concerns for all Agentic AI systems, but as document processing is often perceived to be a simpler task to automate that others, it is important to know a leap forward has occurred, but with it comes a new set of challenges and you will also need a differently skilled team to deal with those new challenges.\\n\\nReal-World Examples we are working currently on:\\n\\n    Finance & Invoicing: Automatically extract and verify financial details directly from invoices.\\n    Legal Contracts: Identify key clauses, obligations, and renewal dates from contracts with full context.\\n    Healthcare Forms: Digitize handwritten forms, medical codes, and spot inconsistencies automatically.\\n    Compliance and Knowledge Management: Feed structured documents into intelligent retrieval systems.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b9977a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=20)\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4dfced4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'C:/Biprayan - 2/LLMops/project/data/demo.txt'}, page_content='Agentic Document Extraction: OCR to Understanding Content\\nAnirban Chakraborty\\nAnirban Chakraborty\\n4 min read\\nÂ·\\nAug 9, 2025'),\n",
       " Document(metadata={'source': 'C:/Biprayan - 2/LLMops/project/data/demo.txt'}, page_content='Press enter or click to view image in full size\\nA diagram showing an early approach at ADE, which will likely evolve over time.'),\n",
       " Document(metadata={'source': 'C:/Biprayan - 2/LLMops/project/data/demo.txt'}, page_content='This article explores how traditional Optical Character Recognition (OCR) has evolved into advanced, generative AI-powered Agentic Document Extraction (ADE) systems â€” capable of genuinely')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df06a177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "71d638dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Agentic Document Extraction: OCR to Understanding Content\\nAnirban Chakraborty\\nAnirban Chakraborty\\n4 min read\\nÂ·\\nAug 9, 2025'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bbf5ada6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.12.10 environment at: C:\\Biprayan - 2\\LLMops\\project\\.venv\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 29ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! uv pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ce2861e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fd7c4aa968545299361f28dfffff0b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acdb6671",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEndpointEmbeddings\n",
    "\n",
    "\n",
    "embedding_function = HuggingFaceEndpointEmbeddings(\n",
    "    model=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    huggingfacehub_api_token= huggingface_api_key\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2074524a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.05422016978263855,\n",
       " 0.08076098561286926,\n",
       " -0.05817992985248566,\n",
       " -0.022709794342517853,\n",
       " 0.0299651101231575,\n",
       " -0.031267620623111725,\n",
       " 0.016452006995677948,\n",
       " -0.02990359626710415,\n",
       " 0.050027474761009216,\n",
       " -0.03307003155350685]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text = [doc.page_content for doc in docs]\n",
    "# embedded_docs = embedding_function.embed_documents(text)\n",
    "# embedded_docs[0][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd95889c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x29af2b46b40>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store = FAISS.from_documents(docs, embedding_function)\n",
    "vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51453007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dfd66c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document : 1\n",
      "Press enter or click to view image in full size\n",
      "A diagram showing an early approach at ADE, which will likely evolve over time.\n",
      "\n",
      "Key Challenges in Agentic Document Extraction\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document : 2\n",
      "Agentic Document Extraction: OCR to Understanding Content\n",
      "Anirban Chakraborty\n",
      "Anirban Chakraborty\n",
      "4 min read\n",
      "Â·\n",
      "Aug 9, 2025\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document : 3\n",
      "This article explores how traditional Optical Character Recognition (OCR) has evolved into advanced, generative AI-powered Agentic Document Extraction (ADE) systems â€” capable of genuinely\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "query = \"what is Agentic Document Extraction\"\n",
    "retrieved_docs = vector_store.similarity_search(query, k=3)\n",
    "\n",
    "for idx, doc in enumerate(retrieved_docs, start=1):\n",
    "    print(\"Document :\", idx)\n",
    "    print(doc.page_content)\n",
    "    print(\"--\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d85efb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
